{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lab Session 9\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk import CFG, ChartParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP (JJ small) (NNS (NNS cats) (CC and) (NNS mice)))\n",
      "(NP (NP (JJ small) (NNS cats)) (CC and) (NP (NNS mice)))\n"
     ]
    }
   ],
   "source": [
    "# Lectures example\n",
    "tokenized_sent = ['small', 'cats', 'and', 'mice']\n",
    "grammar = CFG.fromstring('''\n",
    "    NP -> NNS | JJ NNS | NP CC NP\n",
    "    NNS -> \"cats\" | \"dogs\" | \"mice\" | NNS CC NNS\n",
    "    JJ -> \"big\" | \"small\"\n",
    "    CC -> \"and\" | \"or\"\n",
    "''')\n",
    "parser = ChartParser(grammar)\n",
    "parse = parser.parse(tokenized_sent)\n",
    "for tree in parse:\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (JJ lazy) (NNS cats))\n",
      "  (VP (VBP play) (PP (IN with) (NP (NNS mice)))))\n"
     ]
    }
   ],
   "source": [
    "# Problem example\n",
    "tokenized_sent = ['lazy', 'cats', 'play', 'with', 'mice']\n",
    "grammar = CFG.fromstring('''\n",
    "    S -> NP VP\n",
    "    NP -> JJ NNS\n",
    "    VP -> VBP PP\n",
    "    JJ -> \"lazy\"\n",
    "    NNS -> \"cats\" \n",
    "    VBP -> \"play\"\n",
    "    PP -> IN NP\n",
    "    IN -> \"with\"\n",
    "    NP -> NNS\n",
    "    NNS -> \"mice\"\n",
    "''')\n",
    "parser = ChartParser(grammar)\n",
    "parse = parser.parse(tokenized_sent)\n",
    "for tree in parse:\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['small', 'cats', 'and', 'mice']\n",
      "-------\n",
      "(S (NP (JJ small) (NNS (NNS cats) (CC and) (NNS mice))))\n",
      "-------\n",
      "(S (NP (NP (JJ small) (NNS cats)) (CC and) (NP (NNS mice))))\n",
      "##############\n",
      "['lazy', 'cats', 'play', 'with', 'mice']\n",
      "-------\n",
      "(S\n",
      "  (NP (JJ lazy) (NNS cats))\n",
      "  (VP (VBP play) (PP (IN with) (NP (NNS mice)))))\n",
      "##############\n"
     ]
    }
   ],
   "source": [
    "# Joint grammar example\n",
    "# Expand the grammar in the example of non-probabilistic chart parsers\n",
    "# in order to subsume the sentence:\n",
    "grammar = CFG.fromstring('''\n",
    "    S -> NP VP | NP\n",
    "    VP -> VBP PP\n",
    "    JJ -> \"lazy\" \n",
    "    VBP -> \"play\"\n",
    "    PP -> IN NP\n",
    "    IN -> \"with\"\n",
    "    NP -> NNS | JJ NNS | NP CC NP\n",
    "    NNS -> \"cats\" | \"dogs\" | \"mice\" | NNS CC NNS\n",
    "    JJ -> \"big\" | \"small\"\n",
    "    CC -> \"and\" | \"or\"\n",
    "''')\n",
    "parser = ChartParser(grammar)\n",
    "parse = parser.parse(['small', 'cats', 'and', 'mice'])\n",
    "print(['small', 'cats', 'and', 'mice'])\n",
    "for tree in parse:\n",
    "    print('-------')\n",
    "    print(tree)\n",
    "print('##############')\n",
    "parse = parser.parse(['lazy', 'cats', 'play', 'with', 'mice'])\n",
    "print(['lazy', 'cats', 'play', 'with', 'mice'])\n",
    "for tree in parse:\n",
    "    print('-------')\n",
    "    print(tree)\n",
    "print('##############')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the constituency parsing using a \n",
    "# BottomUpChartParser, BottomUpLeftCornerChartParser & LeftCornerChartParser\n",
    "# For each one of them, provide the resulting tree, the\n",
    "# number of edges and the list of explored edges.\n",
    "\n",
    "import timeit \n",
    "# Wrapper to time parse functions using timit library\n",
    "def wrapper(func, *args):\n",
    "    def wrapped():\n",
    "        return func(*args)\n",
    "    return wrapped\n",
    "\n",
    "# Function to perform parsing\n",
    "def parse(parser, tokenized_sent, timing_iterations):\n",
    "    print('##############')\n",
    "    parse = parser.parse(tokenized_sent)\n",
    "    w = wrapper(parser.parse, tokenized_sent)\n",
    "    parse_time = timeit.timeit(w, number=timing_iterations)\n",
    "    print(tokenized_sent)\n",
    "    print('-------')\n",
    "    print('Resulting tree:')\n",
    "    for tree in parse:\n",
    "        print('-------')\n",
    "        print(tree)\n",
    "    print('##############')\n",
    "    parse = parser.chart_parse(tokenized_sent)\n",
    "    w = wrapper(parser.chart_parse, tokenized_sent)\n",
    "    chart_parse_time = timeit.timeit(w, number=timing_iterations)\n",
    "    edge_count = parse.num_edges()\n",
    "    print('Number of edges:', edge_count)\n",
    "    print('-------')\n",
    "    print('Resulting tree with edges:')\n",
    "    print('-------')\n",
    "    for tree in parse:\n",
    "        print(tree)\n",
    "    print('##############')\n",
    "    return [edge_count, parse_time, chart_parse_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "['lazy', 'cats', 'play', 'with', 'mice']\n",
      "-------\n",
      "Resulting tree:\n",
      "-------\n",
      "(S\n",
      "  (NP (JJ lazy) (NNS cats))\n",
      "  (VP (VBP play) (PP (IN with) (NP (NNS mice)))))\n",
      "##############\n",
      "Number of edges: 52\n",
      "-------\n",
      "Resulting tree with edges:\n",
      "-------\n",
      "[0:1] 'lazy'\n",
      "[1:2] 'cats'\n",
      "[2:3] 'play'\n",
      "[3:4] 'with'\n",
      "[4:5] 'mice'\n",
      "[0:0] JJ -> * 'lazy'\n",
      "[0:1] JJ -> 'lazy' *\n",
      "[0:0] NP -> * JJ NNS\n",
      "[0:1] NP -> JJ * NNS\n",
      "[1:1] NNS -> * 'cats'\n",
      "[1:2] NNS -> 'cats' *\n",
      "[1:1] NP -> * NNS\n",
      "[1:1] NNS -> * NNS CC NNS\n",
      "[0:2] NP -> JJ NNS *\n",
      "[1:2] NP -> NNS *\n",
      "[1:2] NNS -> NNS * CC NNS\n",
      "[1:1] S  -> * NP VP\n",
      "[1:1] S  -> * NP\n",
      "[1:1] NP -> * NP CC NP\n",
      "[1:2] S  -> NP * VP\n",
      "[1:2] S  -> NP *\n",
      "[1:2] NP -> NP * CC NP\n",
      "[0:0] S  -> * NP VP\n",
      "[0:0] S  -> * NP\n",
      "[0:0] NP -> * NP CC NP\n",
      "[0:2] S  -> NP * VP\n",
      "[0:2] S  -> NP *\n",
      "[0:2] NP -> NP * CC NP\n",
      "[2:2] VBP -> * 'play'\n",
      "[2:3] VBP -> 'play' *\n",
      "[2:2] VP -> * VBP PP\n",
      "[2:3] VP -> VBP * PP\n",
      "[3:3] IN -> * 'with'\n",
      "[3:4] IN -> 'with' *\n",
      "[3:3] PP -> * IN NP\n",
      "[3:4] PP -> IN * NP\n",
      "[4:4] NNS -> * 'mice'\n",
      "[4:5] NNS -> 'mice' *\n",
      "[4:4] NP -> * NNS\n",
      "[4:4] NNS -> * NNS CC NNS\n",
      "[4:5] NP -> NNS *\n",
      "[4:5] NNS -> NNS * CC NNS\n",
      "[4:4] S  -> * NP VP\n",
      "[4:4] S  -> * NP\n",
      "[4:4] NP -> * NP CC NP\n",
      "[3:5] PP -> IN NP *\n",
      "[4:5] S  -> NP * VP\n",
      "[4:5] S  -> NP *\n",
      "[4:5] NP -> NP * CC NP\n",
      "[2:5] VP -> VBP PP *\n",
      "[1:5] S  -> NP VP *\n",
      "[0:5] S  -> NP VP *\n",
      "##############\n",
      "##############\n",
      "['lazy', 'cats', 'play', 'with', 'mice']\n",
      "-------\n",
      "Resulting tree:\n",
      "-------\n",
      "(S\n",
      "  (NP (JJ lazy) (NNS cats))\n",
      "  (VP (VBP play) (PP (IN with) (NP (NNS mice)))))\n",
      "##############\n",
      "Number of edges: 31\n",
      "-------\n",
      "Resulting tree with edges:\n",
      "-------\n",
      "[0:1] 'lazy'\n",
      "[1:2] 'cats'\n",
      "[2:3] 'play'\n",
      "[3:4] 'with'\n",
      "[4:5] 'mice'\n",
      "[0:1] JJ -> 'lazy' *\n",
      "[0:1] NP -> JJ * NNS\n",
      "[1:2] NNS -> 'cats' *\n",
      "[1:2] NP -> NNS *\n",
      "[1:2] NNS -> NNS * CC NNS\n",
      "[0:2] NP -> JJ NNS *\n",
      "[0:2] S  -> NP * VP\n",
      "[0:2] S  -> NP *\n",
      "[0:2] NP -> NP * CC NP\n",
      "[1:2] S  -> NP * VP\n",
      "[1:2] S  -> NP *\n",
      "[1:2] NP -> NP * CC NP\n",
      "[2:3] VBP -> 'play' *\n",
      "[2:3] VP -> VBP * PP\n",
      "[3:4] IN -> 'with' *\n",
      "[3:4] PP -> IN * NP\n",
      "[4:5] NNS -> 'mice' *\n",
      "[4:5] NP -> NNS *\n",
      "[4:5] NNS -> NNS * CC NNS\n",
      "[4:5] S  -> NP * VP\n",
      "[4:5] S  -> NP *\n",
      "[4:5] NP -> NP * CC NP\n",
      "[3:5] PP -> IN NP *\n",
      "[2:5] VP -> VBP PP *\n",
      "[0:5] S  -> NP VP *\n",
      "[1:5] S  -> NP VP *\n",
      "##############\n",
      "##############\n",
      "['lazy', 'cats', 'play', 'with', 'mice']\n",
      "-------\n",
      "Resulting tree:\n",
      "-------\n",
      "(S\n",
      "  (NP (JJ lazy) (NNS cats))\n",
      "  (VP (VBP play) (PP (IN with) (NP (NNS mice)))))\n",
      "##############\n",
      "Number of edges: 25\n",
      "-------\n",
      "Resulting tree with edges:\n",
      "-------\n",
      "[0:1] 'lazy'\n",
      "[1:2] 'cats'\n",
      "[2:3] 'play'\n",
      "[3:4] 'with'\n",
      "[4:5] 'mice'\n",
      "[0:1] JJ -> 'lazy' *\n",
      "[0:1] NP -> JJ * NNS\n",
      "[1:2] NNS -> 'cats' *\n",
      "[1:2] NP -> NNS *\n",
      "[0:2] NP -> JJ NNS *\n",
      "[0:2] S  -> NP * VP\n",
      "[0:2] S  -> NP *\n",
      "[1:2] S  -> NP * VP\n",
      "[1:2] S  -> NP *\n",
      "[2:3] VBP -> 'play' *\n",
      "[2:3] VP -> VBP * PP\n",
      "[3:4] IN -> 'with' *\n",
      "[3:4] PP -> IN * NP\n",
      "[4:5] NNS -> 'mice' *\n",
      "[4:5] NP -> NNS *\n",
      "[4:5] S  -> NP *\n",
      "[3:5] PP -> IN NP *\n",
      "[2:5] VP -> VBP PP *\n",
      "[0:5] S  -> NP VP *\n",
      "[1:5] S  -> NP VP *\n",
      "##############\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BottomUp</th>\n",
       "      <th>BottomUpLeftCorner</th>\n",
       "      <th>LeftCorner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>edge_count</th>\n",
       "      <td>52.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parse_time</th>\n",
       "      <td>3.403690</td>\n",
       "      <td>2.319655</td>\n",
       "      <td>1.408021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chart_parse_time</th>\n",
       "      <td>3.264919</td>\n",
       "      <td>3.633334</td>\n",
       "      <td>1.449915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   BottomUp  BottomUpLeftCorner  LeftCorner\n",
       "edge_count        52.000000           31.000000   25.000000\n",
       "parse_time         3.403690            2.319655    1.408021\n",
       "chart_parse_time   3.264919            3.633334    1.449915"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timing_iterations = 1000\n",
    "performance = {}\n",
    "# from nltk.parse.chart import BottomUpChartParser\n",
    "performance['BottomUp'] = parse(BottomUpChartParser(grammar), tokenized_sent, timing_iterations)\n",
    "from nltk.parse.chart import BottomUpLeftCornerChartParser\n",
    "performance['BottomUpLeftCorner'] = parse(BottomUpLeftCornerChartParser(grammar), tokenized_sent, timing_iterations)\n",
    "from nltk.parse.chart import LeftCornerChartParser\n",
    "performance['LeftCorner'] = parse(LeftCornerChartParser(grammar), tokenized_sent, timing_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BottomUp</th>\n",
       "      <th>BottomUpLeftCorner</th>\n",
       "      <th>LeftCorner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>edge_count</th>\n",
       "      <td>52.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parse_time (s)</th>\n",
       "      <td>3.403690</td>\n",
       "      <td>2.319655</td>\n",
       "      <td>1.408021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chart_parse_time (s)</th>\n",
       "      <td>3.264919</td>\n",
       "      <td>3.633334</td>\n",
       "      <td>1.449915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       BottomUp  BottomUpLeftCorner  LeftCorner\n",
       "edge_count            52.000000           31.000000   25.000000\n",
       "parse_time (s)         3.403690            2.319655    1.408021\n",
       "chart_parse_time (s)   3.264919            3.633334    1.449915"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the performance\n",
    "pd.DataFrame(data=performance,index=['edge_count','parse_time (s)','chart_parse_time (s)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Which parser is the most efficient for parsing the sentence? Which edges are\n",
    "# filtered out by each parser and why?\n",
    "\n",
    "# It is observed that BottomUp explores the most edges, \n",
    "# which would explain why it has the highest parse time,\n",
    "# Although it has a lower chart_parse_time than BottomUpLeftCorner.\n",
    "\n",
    "# BottomUpLeftCorner explores 31 edges, more than LeftCorner, although less than BottomUp.\n",
    "# It has a corresponding parse_time greater than LeftCorner but less than BottomUp. It performs\n",
    "# badly at chart_parsing however, this is because of the extra work required to perform the\n",
    "# edge filtering. \n",
    "\n",
    "# It is observed that LeftCorner performs the parse and chart_parse in the least amount of time.\n",
    "# This is because it explores less edges than the other methods.\n",
    "# The LeftCorner only filtered out edges without new word subsumptions, whereas the BottomUpLeft \n",
    "# filters out edges without any word subsumtion, for example, BottomUpLeftCorner has 7 cases of \n",
    "# \"NP ->\", but LeftCorner only has 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dependency parsing. Consider the first three pairs of sentences from the\n",
    "# training set of the evaluation framework of the project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('jumps', 'VBZ') nsubj ('fox', 'NN')\n",
      "('fox', 'NN') det ('The', 'DT')\n",
      "('fox', 'NN') amod ('quick', 'JJ')\n",
      "('fox', 'NN') amod ('brown', 'JJ')\n",
      "('jumps', 'VBZ') nmod ('dog', 'NN')\n",
      "('dog', 'NN') case ('over', 'IN')\n",
      "('dog', 'NN') det ('the', 'DT')\n",
      "('dog', 'NN') amod ('lazy', 'JJ')\n",
      "('jumps', 'VBZ') punct ('.', '.')\n"
     ]
    }
   ],
   "source": [
    "# Corenlp test\n",
    "from nltk.parse import corenlp\n",
    "parser = corenlp.CoreNLPDependencyParser(url='http://localhost:9000')\n",
    "parse, = parser.raw_parse('The quick brown fox jumps over the lazy dog.')\n",
    "for governor, dep, dependent in parse.triples():\n",
    "    print(governor, dep, dependent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sent pair\n",
      "0: Amrozi accused his brother, whom he called \"the witness\", of deliberately distorting his evidence. \n",
      "1:  Referring to him as only \"the witness\", Amrozi accused his brother of deliberately distorting his evidence.\n",
      "\n",
      "\n",
      "Triples_0\n",
      "[(('accused', 'VBD'), 'nsubj', ('Amrozi', 'NNP')), (('accused', 'VBD'), 'dobj', ('brother', 'NN')), (('brother', 'NN'), 'nmod:poss', ('his', 'PRP$')), (('accused', 'VBD'), 'punct', (',', ',')), (('accused', 'VBD'), 'ccomp', ('called', 'VBD')), (('called', 'VBD'), 'dobj', ('whom', 'WP')), (('called', 'VBD'), 'nsubj', ('he', 'PRP')), (('called', 'VBD'), 'punct', ('``', '``')), (('called', 'VBD'), 'dobj', ('witness', 'NN')), (('witness', 'NN'), 'det', ('the', 'DT')), (('called', 'VBD'), 'punct', (\"''\", \"''\")), (('called', 'VBD'), 'punct', (',', ',')), (('called', 'VBD'), 'advcl', ('distorting', 'VBG')), (('distorting', 'VBG'), 'mark', ('of', 'IN')), (('distorting', 'VBG'), 'advmod', ('deliberately', 'RB')), (('distorting', 'VBG'), 'dobj', ('evidence', 'NN')), (('evidence', 'NN'), 'nmod:poss', ('his', 'PRP$')), (('accused', 'VBD'), 'punct', ('.', '.'))]\n",
      "---------------------------\n",
      "\n",
      "Triples_1\n",
      "[(('accused', 'VBD'), 'advcl', ('Referring', 'VBG')), (('Referring', 'VBG'), 'nmod', ('him', 'PRP')), (('him', 'PRP'), 'case', ('to', 'TO')), (('Referring', 'VBG'), 'nmod', ('witness', 'NN')), (('witness', 'NN'), 'case', ('as', 'IN')), (('witness', 'NN'), 'advmod', ('only', 'RB')), (('witness', 'NN'), 'punct', ('``', '``')), (('witness', 'NN'), 'det', ('the', 'DT')), (('witness', 'NN'), 'punct', (\"''\", \"''\")), (('accused', 'VBD'), 'punct', (',', ',')), (('accused', 'VBD'), 'nsubj', ('Amrozi', 'NNP')), (('accused', 'VBD'), 'dobj', ('brother', 'NN')), (('brother', 'NN'), 'nmod:poss', ('his', 'PRP$')), (('accused', 'VBD'), 'advcl', ('distorting', 'VBG')), (('distorting', 'VBG'), 'mark', ('of', 'IN')), (('distorting', 'VBG'), 'advmod', ('deliberately', 'RB')), (('distorting', 'VBG'), 'dobj', ('evidence', 'NN')), (('evidence', 'NN'), 'nmod:poss', ('his', 'PRP$')), (('accused', 'VBD'), 'punct', ('.', '.'))]\n",
      "\n",
      "JACCARD SIMILARITY: 0.37\n",
      "###########################\n",
      "\n",
      "Sent pair\n",
      "0: Yucaipa owned Dominick's before selling the chain to Safeway in 1998 for $2.5 billion. \n",
      "1:  Yucaipa bought Dominick's in 1995 for $693 million and sold it to Safeway for $1.8 billion in 1998.\n",
      "\n",
      "\n",
      "Triples_0\n",
      "[(('owned', 'VBD'), 'nsubj', ('Yucaipa', 'NNP')), (('owned', 'VBD'), 'dobj', ('Dominick', 'NNP')), (('Dominick', 'NNP'), 'case', (\"'s\", 'POS')), (('owned', 'VBD'), 'advcl', ('selling', 'VBG')), (('selling', 'VBG'), 'mark', ('before', 'IN')), (('selling', 'VBG'), 'dobj', ('chain', 'NN')), (('chain', 'NN'), 'det', ('the', 'DT')), (('selling', 'VBG'), 'nmod', ('Safeway', 'NNP')), (('Safeway', 'NNP'), 'case', ('to', 'TO')), (('selling', 'VBG'), 'nmod', ('1998', 'CD')), (('1998', 'CD'), 'case', ('in', 'IN')), (('1998', 'CD'), 'nmod', ('$', '$')), (('$', '$'), 'case', ('for', 'IN')), (('$', '$'), 'nummod', ('billion', 'CD')), (('billion', 'CD'), 'compound', ('2.5', 'CD')), (('owned', 'VBD'), 'punct', ('.', '.'))]\n",
      "---------------------------\n",
      "\n",
      "Triples_1\n",
      "[(('bought', 'VBD'), 'nsubj', ('Yucaipa', 'NNP')), (('bought', 'VBD'), 'dobj', ('Dominick', 'NNP')), (('Dominick', 'NNP'), 'case', (\"'s\", 'POS')), (('bought', 'VBD'), 'nmod', ('1995', 'CD')), (('1995', 'CD'), 'case', ('in', 'IN')), (('1995', 'CD'), 'nmod', ('$', '$')), (('$', '$'), 'case', ('for', 'IN')), (('$', '$'), 'nummod', ('million', 'CD')), (('million', 'CD'), 'compound', ('693', 'CD')), (('bought', 'VBD'), 'cc', ('and', 'CC')), (('bought', 'VBD'), 'conj', ('sold', 'VBD')), (('sold', 'VBD'), 'dobj', ('it', 'PRP')), (('sold', 'VBD'), 'nmod', ('Safeway', 'NNP')), (('Safeway', 'NNP'), 'case', ('to', 'TO')), (('sold', 'VBD'), 'nmod', ('$', '$')), (('$', '$'), 'case', ('for', 'IN')), (('$', '$'), 'nummod', ('billion', 'CD')), (('billion', 'CD'), 'compound', ('1.8', 'CD')), (('$', '$'), 'nmod', ('1998', 'CD')), (('1998', 'CD'), 'case', ('in', 'IN')), (('bought', 'VBD'), 'punct', ('.', '.'))]\n",
      "\n",
      "JACCARD SIMILARITY: 0.161\n",
      "###########################\n",
      "\n",
      "Sent pair\n",
      "0: They had published an advertisement on the Internet on June 10, offering the cargo for sale, he added. \n",
      "1:  On June 10, the ship's owners had published an advertisement on the Internet, offering the explosives for sale.\n",
      "\n",
      "\n",
      "Triples_0\n",
      "[(('published', 'VBN'), 'nsubj', ('They', 'PRP')), (('published', 'VBN'), 'aux', ('had', 'VBD')), (('published', 'VBN'), 'dobj', ('advertisement', 'NN')), (('advertisement', 'NN'), 'det', ('an', 'DT')), (('advertisement', 'NN'), 'nmod', ('Internet', 'NN')), (('Internet', 'NN'), 'case', ('on', 'IN')), (('Internet', 'NN'), 'det', ('the', 'DT')), (('published', 'VBN'), 'nmod', ('June', 'NNP')), (('June', 'NNP'), 'case', ('on', 'IN')), (('June', 'NNP'), 'nummod', ('10', 'CD')), (('published', 'VBN'), 'punct', (',', ',')), (('published', 'VBN'), 'advcl', ('offering', 'VBG')), (('offering', 'VBG'), 'dobj', ('cargo', 'NN')), (('cargo', 'NN'), 'det', ('the', 'DT')), (('cargo', 'NN'), 'nmod', ('sale', 'NN')), (('sale', 'NN'), 'case', ('for', 'IN')), (('published', 'VBN'), 'punct', (',', ',')), (('published', 'VBN'), 'parataxis', ('added', 'VBD')), (('added', 'VBD'), 'nsubj', ('he', 'PRP')), (('published', 'VBN'), 'punct', ('.', '.'))]\n",
      "---------------------------\n",
      "\n",
      "Triples_1\n",
      "[(('published', 'VBN'), 'nmod', ('June', 'NNP')), (('June', 'NNP'), 'case', ('On', 'IN')), (('June', 'NNP'), 'nummod', ('10', 'CD')), (('published', 'VBN'), 'punct', (',', ',')), (('published', 'VBN'), 'nsubj', ('owners', 'NNS')), (('owners', 'NNS'), 'nmod:poss', ('ship', 'NN')), (('ship', 'NN'), 'det', ('the', 'DT')), (('ship', 'NN'), 'case', (\"'s\", 'POS')), (('published', 'VBN'), 'aux', ('had', 'VBD')), (('published', 'VBN'), 'dobj', ('advertisement', 'NN')), (('advertisement', 'NN'), 'det', ('an', 'DT')), (('advertisement', 'NN'), 'nmod', ('Internet', 'NN')), (('Internet', 'NN'), 'case', ('on', 'IN')), (('Internet', 'NN'), 'det', ('the', 'DT')), (('published', 'VBN'), 'punct', (',', ',')), (('published', 'VBN'), 'advcl', ('offering', 'VBG')), (('offering', 'VBG'), 'dobj', ('explosives', 'NNS')), (('explosives', 'NNS'), 'det', ('the', 'DT')), (('offering', 'VBG'), 'nmod', ('sale', 'NN')), (('sale', 'NN'), 'case', ('for', 'IN')), (('published', 'VBN'), 'punct', ('.', '.'))]\n",
      "\n",
      "JACCARD SIMILARITY: 0.444\n",
      "###########################\n"
     ]
    }
   ],
   "source": [
    "# Consider the first three pairs of sentences from the\n",
    "# training set of the evaluation framework of the project. Compute the Jaccard\n",
    "# similarity of each pair using the dependency triples from\n",
    "# CoreNLPDependencyParser.\n",
    "def jaccard_similarity_score(set_1, set_2):\n",
    "    return len(set_1.intersection(set_2)) / float(len(set_1.union(set_2))) \n",
    "\n",
    "with open('IHLT-eval-framework/train/msr_paraphrase_train_input.txt', 'r') as f:\n",
    "    line1, line2, line3 = next(f), next(f), next(f)\n",
    "sent_pairs = [line1.split('\\t'), line2.split('\\t'), line3.split('\\t')]\n",
    "\n",
    "for pair in sent_pairs:\n",
    "    print('\\nSent pair')\n",
    "    print('0:', pair[0])\n",
    "    print('1:', pair[1])\n",
    "    parse_0, = parser.raw_parse(pair[0])\n",
    "    parse_1, = parser.raw_parse(pair[1])\n",
    "    triples_0 = []\n",
    "    for governor, dep, dependent in parse_0.triples():\n",
    "        triples_0.append( (governor, dep, dependent) )\n",
    "    \n",
    "    triples_1 = []\n",
    "    for governor, dep, dependent in parse_1.triples():\n",
    "        triples_1.append( (governor, dep, dependent) )\n",
    "    print('\\nTriples_0')\n",
    "    print(triples_0)\n",
    "    print('---------------------------')\n",
    "    print('\\nTriples_1')\n",
    "    print(triples_1)\n",
    "    print('\\nJACCARD SIMILARITY:', round(jaccard_similarity_score(set(triples_0), set(triples_1)), 3))\n",
    "    print('###########################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
