{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import CoreNLPDependencyParser\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.parse import BllipParser\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.data import find\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.metrics import *\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly.graph_objs import Scatter, Figure, Layout\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    A collection of useful functions for nlp\n",
    "'''\n",
    "# Removing the punctuation and lowering the case of a string\n",
    "def remove_punctuation(line):\n",
    "    return line.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# extract the words from the sentence\n",
    "def words_from_sent(sent):\n",
    "    # tokenized\n",
    "    tokenized = word_tokenize(sent)\n",
    "    # remove stopwords and return\n",
    "    return [word for word in tokenized if word not in stopwords.words('english')]\n",
    "\n",
    "# convert words to tokens\n",
    "def tokens_from_words(words):\n",
    "    return pos_tag(words)\n",
    "\n",
    "# Function to get wordnet pos code\n",
    "def wordnet_pos_code(tag):\n",
    "    if tag.startswith('NN'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('VB'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('JJ'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('RB'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Tokens to lemmas using wordnet lemmatizer    \n",
    "def tokens_to_lemmas(tokens):\n",
    "    return list(map(token_to_lemmas, tokens))\n",
    "\n",
    "def token_to_lemmas(token):    \n",
    "    pos = wordnet_pos_code(token[1])\n",
    "    if pos:\n",
    "        return WordNetLemmatizer().lemmatize(token[0], pos=pos)\n",
    "    return token[0]\n",
    "\n",
    "# Tokens to lemmas and senses (top_synset + pos)\n",
    "def extract_lemmas_senses(tokens):\n",
    "    lemmas, top_synsets = [], []\n",
    "    for token in tokens:\n",
    "        pos = token[1]\n",
    "        wn_pos = wordnet_pos_code(pos)\n",
    "        lemma = WordNetLemmatizer().lemmatize(token[0])\n",
    "        lemmas.append(lemma)\n",
    "        if wn_pos: \n",
    "            synsets = wordnet.synsets(lemma, pos=wn_pos)\n",
    "            if len(synsets) > 0:\n",
    "                top_synsets.append([synsets[0], pos])\n",
    "    return lemmas, top_synsets\n",
    "\n",
    "# Load the lines of training text as sentences\n",
    "def text_to_sentences(filename):\n",
    "    sentence_pair_array = []\n",
    "    for line in open(filename, encoding=\"UTF8\").readlines():\n",
    "        sentence_pair_array.append([s.strip() for s in line.split(\"\\t\")])\n",
    "    return sentence_pair_array\n",
    "\n",
    "def compare_synsets(synset_a, synset_b):\n",
    "    lcs = synset_a.lowest_common_hypernyms(synset_b)\n",
    "    similarity = synset_a.path_similarity(synset_b)\n",
    "    wup_similarity = synset_a.wup_similarity(synset_b)\n",
    "    lin_similarity = synset_a.lin_similarity(synset_b, brown_ic)                        \n",
    "    lch_similarity = synset_a.lch_similarity(synset_b)\n",
    "    return lcs, similarity, wup_similarity, lin_similarity, lch_similarity\n",
    "\n",
    "def count(g,s):\n",
    "    TP = TN = FP = FN = 0\n",
    "    for i in range(0,len(g)):\n",
    "        if (g[i]==s[i] and s[i]==1): TP+=1\n",
    "        if (g[i]==s[i] and s[i]==0): TN+=1\n",
    "        if (g[i]!=s[i] and s[i]==1): FP+=1\n",
    "        if (g[i]!=s[i] and s[i]==0): FN+=1\n",
    "    return [TP,TN,FP,FN]\n",
    "    \n",
    "def MSRP_eval(gs, sys):\n",
    "    [TP,TN,FP,FN] = count(gs,sys)\n",
    "    acc = (TP+TN)/float(TP+TN+FP+FN) # ACCURACY\n",
    "    reject = TN/float(TN+FP) # precision on negative SPECIFICITY\n",
    "    accept = TP/float(TP+FN) # precision on positive SENSITIVITY\n",
    "    print(\"acc=\",acc,\" reject=\",reject,\" accept=\",accept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    The paraphrase data that will be analysed\n",
    "'''\n",
    "train_input = text_to_sentences('../IHLT-eval-framework/train/msr_paraphrase_train_input.txt')\n",
    "test_input = text_to_sentences('../IHLT-eval-framework/test/msr_paraphrase_test_input.txt')\n",
    "train_classes = open('../IHLT-eval-framework/train/msr_paraphrase_train_gs.txt', encoding=\"utf-8-sig\").readlines()\n",
    "test_classes = open('../IHLT-eval-framework/test/msr_paraphrase_test_gs.txt', encoding=\"UTF8\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "..............................................................................................."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-208d564d45a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlemma_jaccard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_classes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Testing'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-208d564d45a7>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlemma_jaccard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_classes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Testing'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-208d564d45a7>\u001b[0m in \u001b[0;36mlemma_jaccard\u001b[0;34m(sent_0, sent_1)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msent_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_punctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_punctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mwords_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords_from_sent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_from_sent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtokens_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens_from_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens_from_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlemmas_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemmas_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens_to_lemmas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens_to_lemmas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-6aa48e10a1d3>\u001b[0m in \u001b[0;36mwords_from_sent\u001b[0;34m(sent)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# remove stopwords and return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# convert words to tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-6aa48e10a1d3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# remove stopwords and return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# convert words to tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/python3/lib/python3.6/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \"\"\"\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         return [line for line in line_tokenize(self.raw(fileids))\n\u001b[0m\u001b[1;32m     23\u001b[0m                 if not line.startswith(ignore_lines_startswith)]\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/python3/lib/python3.6/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mraw\u001b[0;34m(self, fileids)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/python3/lib/python3.6/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "    First Paraphrase detector approach, jaccard distance of lemmas    \n",
    "'''\n",
    "def lemma_jaccard(sent_0, sent_1):\n",
    "    print('.', end='')\n",
    "    sent_0, sent_1 = remove_punctuation(sent_0.lower()), remove_punctuation(sent_1.lower())\n",
    "    words_0, words_1 = words_from_sent(sent_0), words_from_sent(sent_1)\n",
    "    tokens_0, tokens_1 = tokens_from_words(words_0), tokens_from_words(words_1)\n",
    "    lemmas_0, lemmas_1 = tokens_to_lemmas(tokens_0), tokens_to_lemmas(tokens_1)\n",
    "    return jaccard_distance(set(lemmas_0), set(lemmas_1))\n",
    "\n",
    "print('Training')\n",
    "X_train = [lemma_jaccard(data[0], data[1]) for data in train_input]\n",
    "y_train = [int(line.strip()) for line in train_classes]\n",
    "print('Testing')\n",
    "X_test = [lemma_jaccard(data[0], data[1])for data in test_input]\n",
    "y_test = [int(line.strip()) for line in test_classes]\n",
    "print('Results')\n",
    "regression = LogisticRegression()\n",
    "regression.fit(np.array(X_train).reshape(-1,1), y_train)\n",
    "prediction = regression.predict(np.array(X_test).reshape(-1,1))\n",
    "MSRP_eval(prediction, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................Testing\n",
      ".............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................Results\n",
      "acc= 0.7107246376811595  reject= 0.6118980169971672  accept= 0.7361516034985423\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Second Paraphrase detector approach, lemma jaccard + top synset pos jaccard \n",
    "'''\n",
    "def lemma_pos_jaccard(sent_0, sent_1):\n",
    "    print('.', end='')\n",
    "    sent_0, sent_1 = remove_punctuation(sent_0.lower()), remove_punctuation(sent_1.lower())\n",
    "    words_0, words_1 = words_from_sent(sent_0), words_from_sent(sent_1)\n",
    "    tokens_0, tokens_1 = tokens_from_words(words_0), tokens_from_words(words_1)\n",
    "    lemmas_0, senses_0 = extract_lemmas_senses(tokens_0)\n",
    "    lemmas_1, senses_1 = extract_lemmas_senses(tokens_1)                        \n",
    "    top_synsets_pos_0 = [sense[1] for sense in senses_0]\n",
    "    top_synsets_pos_1 = [sense[1] for sense in senses_1]\n",
    "    return [\n",
    "        jaccard_distance(set(lemmas_0), set(lemmas_1)),\n",
    "        jaccard_distance(set(top_synsets_pos_0), set(top_synsets_pos_1))\n",
    "    ]\n",
    "\n",
    "print('Training')\n",
    "X_train = [lemma_pos_jaccard(data[0], data[1]) for data in train_input]\n",
    "y_train = [int(line.strip()) for line in train_classes]\n",
    "print('Testing')\n",
    "X_test = [lemma_pos_jaccard(data[0], data[1])for data in test_input]\n",
    "y_test = [int(line.strip()) for line in test_classes]\n",
    "print('Results')\n",
    "regression = LogisticRegression()\n",
    "regression.fit(np.array(X_train), y_train)\n",
    "prediction = regression.predict(np.array(X_test))\n",
    "MSRP_eval(prediction, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................Testing\n",
      ".............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................Results\n",
      "acc= 0.7101449275362319  reject= 0.6681034482758621  accept= 0.7166778298727394\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Third Paraphrase detector approach, lemma jaccard + top synset pos jaccard + synset distance\n",
    "'''\n",
    "def lemma_pos_jaccard(sent_0, sent_1):\n",
    "    print('.', end='')\n",
    "    sent_0, sent_1 = remove_punctuation(sent_0.lower()), remove_punctuation(sent_1.lower())\n",
    "    words_0, words_1 = words_from_sent(sent_0), words_from_sent(sent_1)    \n",
    "    tokens_0, tokens_1 = tokens_from_words(words_0), tokens_from_words(words_1)\n",
    "    lemmas_0, senses_0 = extract_lemmas_senses(tokens_0)\n",
    "    lemmas_1, senses_1 = extract_lemmas_senses(tokens_1)                        \n",
    "    top_synsets_pos_0, top_synsets_pos_1 = [], []\n",
    "    # grouping the similarity by pos     \n",
    "    noun_distance, verb_distance, adj_distance, adv_distance = [], [], [], []\n",
    "    for sense_0 in senses_0:        \n",
    "        # add the pos tag (NOT the wordnet one) to the top synset pos list)\n",
    "        top_synsets_pos_0.append(sense_0[1])\n",
    "        for sense_1 in senses_1:\n",
    "            # add the pos tag (NOT the wordnet one) to the top synset pos list)         \n",
    "            top_synsets_pos_1.append(sense_1[1])\n",
    "            #  compare wordnet pos tags and calculate path similarity \n",
    "            if ((sense_0[0].pos() == 'n') & (sense_1[0].pos() == 'n')):\n",
    "                noun_distance.append(sense_0[0].wup_similarity(sense_1[0]))\n",
    "            if ((sense_0[0].pos() == 'v') & (sense_1[0].pos() == 'v')):\n",
    "                verb_distance.append(sense_0[0].wup_similarity(sense_1[0]))\n",
    "            if ((sense_0[0].pos() == 'a') & (sense_1[0].pos() == 'a')):\n",
    "                adj_distance.append(sense_0[0].wup_similarity(sense_1[0]))\n",
    "            if ((sense_0[0].pos() == 'r') & (sense_1[0].pos() == 'r')):\n",
    "                adv_distance.append(sense_0[0].wup_similarity(sense_1[0]))\n",
    "    noun_distance = np.sum(list(filter(None, noun_distance)))\n",
    "    verb_distance = np.sum(list(filter(None, verb_distance)))\n",
    "    adj_distance = np.sum(list(filter(None, adj_distance))) \n",
    "    adv_distance = np.sum(list(filter(None, adv_distance)))\n",
    "    return [\n",
    "        jaccard_distance(set(lemmas_0), set(lemmas_1)),\n",
    "        jaccard_distance(set(top_synsets_pos_0), set(top_synsets_pos_1)),\n",
    "        noun_distance, \n",
    "        verb_distance,\n",
    "        adj_distance, \n",
    "        adv_distance\n",
    "    ]\n",
    "\n",
    "print('Training')\n",
    "X_train = [lemma_pos_jaccard(data[0], data[1]) for data in train_input]\n",
    "y_train = [int(line.strip()) for line in train_classes]\n",
    "print('Testing')\n",
    "X_test = [lemma_pos_jaccard(data[0], data[1])for data in test_input]\n",
    "y_test = [int(line.strip()) for line in test_classes]\n",
    "print('Results')\n",
    "classifier = LinearSVC(penalty='l2', loss='hinge')\n",
    "classifier.fit(np.array(X_train), y_train)\n",
    "prediction = classifier.predict(np.array(X_test))\n",
    "MSRP_eval(prediction, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Jjaccard distance of lemmas + Dependency parser triples  \n",
    "'''\n",
    "def jaccard_similarity_score(set_1, set_2):\n",
    "    return len(set_1.intersection(set_2)) / float(len(set_1.union(set_2))) \n",
    "\n",
    "def lemma_jaccard_triples(sent_0, sent_1):\n",
    "    print('.', end='')\n",
    "    sent_0, sent_1 = remove_punctuation(sent_0.lower()), remove_punctuation(sent_1.lower())\n",
    "    words_0, words_1 = words_from_sent(sent_0), words_from_sent(sent_1)\n",
    "    tokens_0, tokens_1 = tokens_from_words(words_0), tokens_from_words(words_1)\n",
    "    lemmas_0, lemmas_1 = tokens_to_lemmas(tokens_0), tokens_to_lemmas(tokens_1)\n",
    "    dep_parser = CoreNLPDependencyParser(url='http://localhost:9000')    \n",
    "#     try:\n",
    "    pars1, = dep_parser.raw_parse(sent_0)\n",
    "    pars2, = dep_parser.raw_parse(sent_1)\n",
    "    triples_0 = []\n",
    "    for governor, dep, dependent in parse_0.triples():\n",
    "        triples_0.append( (governor, dep, dependent) )\n",
    "\n",
    "    triples_1 = []\n",
    "    for governor, dep, dependent in parse_1.triples():\n",
    "        triples_1.append( (governor, dep, dependent) )\n",
    "\n",
    "    return jaccard_similarity_score(set(triples_0), set(triples_1))\n",
    "#     except:\n",
    "#         print('Error\\n', sent_0, '\\n', sent_1)\n",
    "#         return 0.5\n",
    "    \n",
    "\n",
    "print('Training')\n",
    "X_train = [lemma_jaccard_triples(data[0], data[1]) for data in train_input]\n",
    "y_train = [int(line.strip()) for line in train_classes]\n",
    "print('Testing')\n",
    "X_test = [lemma_jaccard_triples(data[0], data[1])for data in test_input]\n",
    "y_test = [int(line.strip()) for line in test_classes]\n",
    "print('Results')\n",
    "classifier = LinearSVC(penalty='l2', loss='hinge')\n",
    "classifier.fit(np.array(X_train), y_train)\n",
    "prediction = classifier.predict(np.array(X_test))\n",
    "MSRP_eval(prediction, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
